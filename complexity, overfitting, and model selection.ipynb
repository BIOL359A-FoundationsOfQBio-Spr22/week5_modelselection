{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Complexity, Overfitting, and Model Selection\n",
    "### Spring 2022, Week 5\n",
    "<hr>\n",
    "\n",
    "Objectives:\n",
    "-  Develop and run increasingly complex MLR models\n",
    "-  Evaluate performance of MLR models based on Cross-Validation\n",
    "-  Evaluate other supervised learning models based on Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr22/week5_modelselection\n",
    "!mkdir ./data\n",
    "!cp week5_modelselection/data/* ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (10,10)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':FIG_SIZE}) \n",
    "sns.set_style(\"whitegrid\",  {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "plt.rc(\"axes.spines\", top=False, right=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature Name | Description | \n",
    "| --- | --- |\n",
    "| Target seq | target mRNA GenBank sequence accession number |\n",
    "| Start | start position on target mRNA | \n",
    "| End | end position on target mRNA | \n",
    "| Sequence | siRNA sequence |\n",
    "| G | nucleotide content, G |\n",
    "| U | nucleotide content, U | \n",
    "| bi | stability (∆G) of dimers of siRNAs antisense strands |  \n",
    "| uni | siRNA antisense strand intra-molecular structure stability (∆G) |\n",
    "| duplex | ∆G of sense-antisense siRNA duplexes | \n",
    "| Pos1 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 1 | \n",
    "| Pos2 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 2 | \n",
    "| Pos6 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 6 | \n",
    "| Pos13 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 13 | \n",
    "| Pos14 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 14 | \n",
    "| Pos18 | stability profile (∆G) of each two neighboring base pairs in the siRNA sense-antisense, position 18 | \n",
    "| Dif_5-3 | ∆G difference between position 1 and 18 |\n",
    "| Content+ | preferred dinucleotide content |\n",
    "| Content- | avoided dinucleotide content | \n",
    "| Cons+ | position-dependent nucleotide consensus, preferred |\n",
    "| Cons- | position-dependent nucleotide consensus, avoided |\n",
    "| Cons_Sum | position-dependent nucleotide consensus, sum |\n",
    "| Hyb19 | number of potential target copies in mRNAs (∆G threshold) |\n",
    "| target | local target mRNA stabilities (∆G) |\n",
    "| Activity | gene expression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2-3: Dataset\n",
    "We are going to take out the non-numeric features. There are a couple thigns that we could potentially do with them, but that type of feature engineering is outside the scope of this class. __Refer to the table below to answer questions 2 and 3.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    num_df = df.select_dtypes(include=numerics)\n",
    "    return num_df\n",
    "    \n",
    "num_dataset = get_numerical(dataset)\n",
    "num_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dataset.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5-8: Training Data and Test Data\n",
    "\n",
    "When viewing datasets, you can treat every feature as a __dimension__ (think about how each feature can be represented on an axis). Below are PCA (Principal Component Analysis) plots, which are a way to visualize high-dimensional datasets in fewer dimensions. We will cover this analysis later in the course, but if points are close on a PCA plot, it generally means they contain similar information.\n",
    "\n",
    "As a researcher, you may need to take a look at your training/test split to see if you are including any bias in your training/test data. Below are 4 different PCA plots, with training-test splits. Which are good? Which are not good? What type of bias would you introduce? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def performPCA(X, n_dimensions=2, drop_columns=[\"Start\", \"End\"]):\n",
    "    \"\"\"\n",
    "    Uses sklearn PCA tool to perform PCA\n",
    "    input:\n",
    "    X: Pandas Dataframe or Numpy Array of features\n",
    "    n_dimensions: Number of PCs to fit\n",
    "    \n",
    "    output:\n",
    "    X_pca: Pandas dataframe with column titles of PC1,...,PCn\n",
    "    \"\"\"\n",
    "    X_data = get_numerical(X)\n",
    "    if drop_columns: X_data.drop([], axis=1)\n",
    "    X_standardized = StandardScaler().fit_transform(X_data)\n",
    "    pca = PCA(n_components=n_dimensions)\n",
    "    pca.fit(X_standardized)\n",
    "    X_pca_array = pca.transform(X_standardized)\n",
    "    column_names = ['PC{}'.format(i+1) for i in range(n_dimensions)] \n",
    "    X_pca = pd.DataFrame(X_pca_array, columns=column_names)\n",
    "    return X_pca\n",
    "\n",
    "def makeplots(pca_data):\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data, color=\"grey\",alpha=.3, label=\"Training\")\n",
    "    biased_A = pca_data[pca_data[\"PC1\"]>-1]\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=biased_A.sample(130), color=\"blue\", label=\"Test\")\n",
    "    plt.title(\"A: ($n_{test}=130$)\", fontweight=\"bold\")\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data, color=\"grey\",alpha=.3, label=\"Training\")\n",
    "    biased_B = pca_data[pca_data[\"PC2\"]<1.5]\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=biased_B.sample(130), color=\"orange\", label=\"Test\")\n",
    "    plt.title(\"B: ($n_{test}=130$)\", fontweight=\"bold\")\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data, color=\"grey\",alpha=.3, label=\"Training\")\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data.sample(130), color=\"purple\", label=\"Test\")\n",
    "    plt.title(\"C: ($n_{test}=130$)\", fontweight=\"bold\")\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data, color=\"grey\",alpha=.3, label=\"Training\")\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_data.sample(40), color=\"red\", label=\"Test\")\n",
    "    plt.title(r\"D: ($n_{test}=40$)\", fontweight=\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "pca = performPCA(dataset)\n",
    "makeplots(pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR review:\n",
    "\n",
    "We will perform a normal, uncorrected MLR on the quantitative variables in the dataset. _Try to interpret the variables. Does anything seem odd? Compare the coefficients to the original ranges and typical values of the features they are associated with. What features would you take out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def linear_regression(df, feature_cols, response_col, standardized = False, print_coef=True):\n",
    "    \"\"\"\n",
    "    Use linear_model to run a linear regression using sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    X = df[feature_cols]\n",
    "    y = df[response_col]\n",
    "    if standardized:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        y = StandardScaler().fit_transform(y.values.reshape(-1, 1))\n",
    "    regression = linear_model.LinearRegression() \n",
    "    regression.fit(X,y)\n",
    "    if print_coef:\n",
    "        try:\n",
    "            print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_))\n",
    "        except TypeError:\n",
    "            print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_[0]))\n",
    "        print('Regression Coefficients: ')\n",
    "        for feature, coef in zip(feature_cols, regression.coef_.flatten()):\n",
    "            print(f'{feature} ~ {coef:.2f}')\n",
    "    return regression.predict(X), regression.score(X,y)\n",
    "\n",
    "def parity_plot(true, pred, r_squared=None, title='', alpha=None, color=None, hue=None):\n",
    "    \"\"\"\n",
    "    plot true vs the predicted data\n",
    "    inputs: 2 list-like (arrays) data structures\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "    if hue is not None:\n",
    "        sns.scatterplot(x=true, y=pred, hue=hue)\n",
    "    else: \n",
    "        if color is None: sns.scatterplot(x=true, y=pred)\n",
    "        else: sns.scatterplot(x=true, y=pred, alpha=alpha, color=color)\n",
    "    min_value = min(min(true), min(pred))\n",
    "    max_value = max(max(true), max(pred))\n",
    "    plt.plot([min_value, max_value],[min_value, max_value], '--', label=\"parity\")\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    ax.set_box_aspect(1)\n",
    "    sns.despine()\n",
    "    plt.text(1.01, 0.98, r\"$R^2 = {0:.2f}$\".format(r_squared),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    plt.title('Parity Plot: {}'.format(title), size=TITLE_FONT)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "    \n",
    "def run_regression(data, \n",
    "                   feature_cols = ['Start', \n",
    "                                   'End', \n",
    "                                   'G', \n",
    "                                   'U', \n",
    "                                   'bi', \n",
    "                                   'uni', \n",
    "                                   'duplex', \n",
    "                                   'Pos1', \n",
    "                                   'Pos2', \n",
    "                                   'Pos6', \n",
    "                                   'Pos13', \n",
    "                                   'Pos14', \n",
    "                                   'Pos18', \n",
    "                                   'Dif_5-3', \n",
    "                                   'Content+', \n",
    "                                   'Content-', \n",
    "                                   'Cons+', \n",
    "                                   'Cons-', \n",
    "                                   'Cons_Sum', \n",
    "                                   'Hyb19', \n",
    "                                   'target'\n",
    "                                  ], \n",
    "                     response_col='Activity',\n",
    "                     standardized=False,\n",
    "                     parity=True,\n",
    "                     ):\n",
    "    y_pred, r_squared = linear_regression(data, feature_cols, response_col, standardized = standardized)\n",
    "    if parity: parity_plot(data[response_col], y_pred.flatten(), r_squared)\n",
    "\n",
    "run_regression(data=num_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact()   \n",
    "def regression_wrapper(Start = True,\n",
    "                       End = True,\n",
    "                       G = True,\n",
    "                       U = True,\n",
    "                       bi = True,\n",
    "                       uni = True,\n",
    "                       duplex = True,\n",
    "                       Pos1 = True,\n",
    "                       Pos2 = True,\n",
    "                       Pos6 = True,\n",
    "                       Pos13 = True,\n",
    "                       Pos14 = True,\n",
    "                       Pos18 = True,\n",
    "                       Dif_5_3 = True,\n",
    "                       Content_plus = True,\n",
    "                       Content_minus = True,\n",
    "                       Cons_plus = True,\n",
    "                       Cons_minus = True,\n",
    "                       Cons_Sum = True,\n",
    "                       Hyb19 = True,\n",
    "                       target = True):\n",
    "    response=\"Activity\"\n",
    "    features = []\n",
    "    if Start: features.append('Start')\n",
    "    if End: features.append('End')\n",
    "    if G: features.append('G')\n",
    "    if U: features.append('U')\n",
    "    if bi: features.append('bi')\n",
    "    if uni: features.append('uni')\n",
    "    if duplex: features.append('duplex')\n",
    "    if Pos1: features.append('Pos1')\n",
    "    if Pos2: features.append('Pos2')\n",
    "    if Pos6: features.append('Pos6')\n",
    "    if Pos13: features.append('Pos13')\n",
    "    if Pos14: features.append('Pos14')\n",
    "    if Pos18: features.append('Pos18')\n",
    "    if Dif_5_3: features.append('Dif_5-3')\n",
    "    if Content_plus: features.append('Content+')\n",
    "    if Content_minus: features.append('Content-')\n",
    "    if Cons_plus: features.append('Cons+')\n",
    "    if Cons_minus: features.append('Cons-')\n",
    "    if Cons_Sum: features.append('Cons_Sum')\n",
    "    if Hyb19: features.append('Hyb19')\n",
    "    if target: features.append('target')        \n",
    "\n",
    "    run_regression(data=num_dataset, feature_cols=features, response_col = response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make a better model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y, k=10):\n",
    "    scores = cross_validate(model, X, y, cv=k,\n",
    "                            scoring=('r2', 'neg_root_mean_squared_error'),\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    print(f\"Training RMSE: {np.mean(-1*scores['train_neg_root_mean_squared_error']):.2f}\")\n",
    "    print(f\"Validation RMSE: {np.mean(-1*scores['test_neg_root_mean_squared_error']):.2f}\")\n",
    "    \n",
    "    print(f\"Training R2: {np.mean(scores['train_r2']):.3f}\")\n",
    "    print(f\"Validation R2: {np.mean(scores['test_r2']):.3f}\")\n",
    "    \n",
    "    return model.fit(X,y)\n",
    "    \n",
    "def do_the_thing(X,y , degrees, interactions, test):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    poly = PolynomialFeatures(degrees, include_bias=True)\n",
    "    # print(poly.get_feature_names_out(input_features=X_train.columns))\n",
    "    X_train = poly.fit_transform(X_train)\n",
    "    poly_features = poly.get_feature_names_out()\n",
    "    X_test = poly.fit_transform(X_test, poly_features)\n",
    "    X_train_df = pd.DataFrame(X_train, columns=poly_features)\n",
    "    X_test_df = pd.DataFrame(X_train, columns=poly_features)\n",
    "    interaction_list = [feat for feat in poly_features if len(feat.split())!=1]\n",
    "    if not interactions:\n",
    "        X_train_df = X_train_df.drop(interaction_list, axis=1)\n",
    "        X_test_df = X_test_df.drop(interaction_list, axis=1)\n",
    "    print(f\"Number of Parameters: {len(X_train_df.columns)}\")\n",
    "    \n",
    "    model = linear_model.LinearRegression(fit_intercept=False) \n",
    "    cross_validation(model, X_train_df, y_train)\n",
    "    if test:\n",
    "        model.fit(X_train_df, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        parity_plot(y_train, y_train_pred.flatten(), r_squared =model.score(X_train, y_train), title=\"Training Data\", color=\"grey\", alpha=0.5)\n",
    "        parity_plot(y_test, y_test_pred.flatten(), r_squared =model.score(X_test, y_test), title=\"Test Data\", color=\"blue\", alpha=1)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "@widgets.interact(degrees=(1,4))\n",
    "def run(degrees=1, \n",
    "        interactions=False,\n",
    "        test=False,\n",
    "        Start = True,\n",
    "        End = True,\n",
    "        G = True,\n",
    "        U = True,\n",
    "        bi = True,\n",
    "        uni = True,\n",
    "        duplex = True,\n",
    "        Pos1 = True,\n",
    "        Pos2 = True,\n",
    "        Pos6 = True,\n",
    "        Pos13 = True,\n",
    "        Pos14 = True,\n",
    "        Pos18 = True,\n",
    "        Dif_5_3 = True,\n",
    "        Content_plus = True,\n",
    "        Content_minus = True,\n",
    "        Cons_plus = True,\n",
    "        Cons_minus = True,\n",
    "        Cons_Sum = True,\n",
    "        Hyb19 = True,\n",
    "        target = True):\n",
    "    \n",
    "    response=\"Activity\"\n",
    "    features = []\n",
    "    if Start: features.append('Start')\n",
    "    if End: features.append('End')\n",
    "    if G: features.append('G')\n",
    "    if U: features.append('U')\n",
    "    if bi: features.append('bi')\n",
    "    if uni: features.append('uni')\n",
    "    if duplex: features.append('duplex')\n",
    "    if Pos1: features.append('Pos1')\n",
    "    if Pos2: features.append('Pos2')\n",
    "    if Pos6: features.append('Pos6')\n",
    "    if Pos13: features.append('Pos13')\n",
    "    if Pos14: features.append('Pos14')\n",
    "    if Pos18: features.append('Pos18')\n",
    "    if Dif_5_3: features.append('Dif_5-3')\n",
    "    if Content_plus: features.append('Content+')\n",
    "    if Content_minus: features.append('Content-')\n",
    "    if Cons_plus: features.append('Cons+')\n",
    "    if Cons_minus: features.append('Cons-')\n",
    "    if Cons_Sum: features.append('Cons_Sum')\n",
    "    if Hyb19: features.append('Hyb19')\n",
    "    if target: features.append('target')  \n",
    "    X=num_dataset[features]\n",
    "    y=num_dataset[response]\n",
    "    \n",
    "\n",
    "    do_the_thing(X,y,degrees,interactions, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "from sklearn import ensemble, neural_network, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rf(X,y):\n",
    "    print('-'*30)\n",
    "    print(\"Random Forest (boosted trees)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = ensemble.GradientBoostingRegressor(loss='absolute_error', learning_rate=.05, alpha=0.2)\n",
    "    cross_validation(model, X_train, y_train)\n",
    "    model.fit(X_train,y_train)\n",
    "    r2_test = model.score(X_test,y_test)\n",
    "    print(f\"\\nTest R2: {r2_test:.2f}\")\n",
    "    model.fit(X,y)\n",
    "    r2 = model.score(X,y)\n",
    "    y_pred = model.predict(X )\n",
    "    parity_plot(y, y_pred, r_squared=r2,   title=\"RF, all data\")\n",
    "    \n",
    "def svr(X,y):\n",
    "    print('-'*30)\n",
    "    print(\"Support Vector Regression\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = svm.SVR(kernel=\"rbf\",gamma=\"scale\", C=1000)\n",
    "    cross_validation(model, X_train, y_train)\n",
    "    model.fit(X_train,y_train)\n",
    "    r2_test = model.score(X_test,y_test)\n",
    "    print(f\"\\nTest R2: {r2_test:.2f}\")\n",
    "    model.fit(X,y)\n",
    "    r2 = model.score(X,y)\n",
    "    y_pred = model.predict(X )\n",
    "    parity_plot(y, y_pred, r_squared=r2,  title=\"SVR, all data\")\n",
    "    \n",
    "def nn(X,y):\n",
    "    print('-'*30)\n",
    "    print(\"Neural Network Regression (Multi-layer perceptron)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = neural_network.MLPRegressor(hidden_layer_sizes=(50,30),\n",
    "                                        learning_rate='invscaling',\n",
    "                                        activation=\"relu\", \n",
    "                                        max_iter=1000)\n",
    "    cross_validation(model, X_train, y_train)\n",
    "    model.fit(X_train,y_train)\n",
    "    r2_test = model.score(X_test,y_test)\n",
    "    print(f\"\\nTest R2: {r2_test:.2f}\")\n",
    "    model.fit(X,y)\n",
    "    r2 = model.score(X,y)\n",
    "    y_pred = model.predict(X )\n",
    "    parity_plot(y, y_pred, r_squared=r2, title=\"NN, all data\")\n",
    "    \n",
    "features = ['bi', 'uni', 'duplex', \n",
    "            'Pos1', 'Pos2', 'Pos6', 'Pos13', 'Pos14', 'Pos18', \n",
    "            'Dif_5-3', 'Content+', \n",
    "            'Cons+', 'Cons-', 'Cons_Sum', 'Hyb19', 'target']\n",
    "\n",
    "X= num_dataset[features]\n",
    "y= num_dataset[\"Activity\"]\n",
    "\n",
    "rf(X,y)\n",
    "svr(X,y)\n",
    "nn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
